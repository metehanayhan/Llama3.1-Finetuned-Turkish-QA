# Llama3.1 Turkish Educational ChatBot - Metehan Ayhan

[EN]

## About the Project

This project is a fine-tuned version of the **Meta LLaMA 3.1 8B** large language model, specifically adapted to respond to **Turkish educational question-answer** prompts. The main goal is to deliver fluent, informative, and context-aware answers in Turkish, suitable for general inquiry and learning support.

The model was fine-tuned using the **LoRA** technique on a small scale (1% of trainable parameters) and published on Hugging Face:

ğŸ”— [metehanayhan/Llama3-1_Turkish_ChatBot](https://huggingface.co/metehanayhan/Llama3-1_Turkish_ChatBot)

---

## Training Summary

| Feature | Value |
| --- | --- |
| Base Model | Meta LLaMA 3.1 8B |
| Fine-Tuning Method | Supervised Fine-Tuning (SFT) |
| LoRA Usage | Yes (%1 of model trained) |
| Training Data | Turkish questionâ€“answer pairs |
| Number of Training Samples | 17,587 |
| Epochs | 1 |
| Total Training Steps | 2,199 |
| Learning Rate | 2e-5 |
| Total Batch Size | 8 |
| Training Duration (approx.) | ~3 hours |
| Trainable Parameters | 83M / 8B (1.05%) |
| Quantization | 4-bit |

---

## Data Format

The dataset consists of Turkish questionâ€“answer pairs provided in CSV format. Each row represents a complete educational sample.

Example:

```
question,answer
What can be done to prevent climate change?,
"To combat climate change, actions like reducing fossil fuel usage and transitioning to renewable energy sources are essential..."
```

A total of 17,587 such examples were used for fine-tuning.

---

## Use Case

This model is intended to serve as an educational assistant in Turkish. It can answer questions related to:

- Informative, general-knowledge, or school-related topics
- Support for curious learners and students

---

## Quick Start

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("metehanayhan/Llama3-1_Turkish_ChatBot")
model = AutoModelForCausalLM.from_pretrained("metehanayhan/Llama3-1_Turkish_ChatBot")

qa_pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
print(qa_pipe("Ä°klim deÄŸiÅŸikliÄŸi neden Ã¶nemlidir?", max_new_tokens=200)[0]["generated_text"])
```

---

## Performance Notes

The model performs well on Turkish QA-style prompts that resemble the training distribution:

- ğŸ”¸ Fluent and natural Turkish sentence construction
- ğŸ”¸ Answers are contextually aligned with the prompt

The model shows strong generalization, even with limited training, due to the LoRA technique and high-quality data.

<p align="center">
  <img src="image.png" width="600"/>
</p>


---

## Deployment

The model is shared on Hugging Face with 4-bit quantization and is ready for low-resource inference. It has also been exported in GGUF format for use in compatible environments.

---

## Additional Notes

- The training was performed using `Trainer` with standard SFT configuration.
- `random_state = 3407` was used to ensure reproducibility.
- Although fine-tuned on just 1% of parameters, the model responds effectively across a wide range of Turkish topics.

---

[TR]

# Llama3.1 TÃ¼rkÃ§e EÄŸitim OdaklÄ± ChatBot

## Proje HakkÄ±nda

Bu model, **Meta LLaMA 3.1 8B** tabanlÄ± bÃ¼yÃ¼k bir dil modelidir ve TÃ¼rkÃ§e dilinde, eÄŸitim odaklÄ± **soru-cevap (QA)** verisiyle fine-tune edilmiÅŸtir. AmaÃ§, kullanÄ±cÄ±larÄ±n bilgi arayÄ±ÅŸÄ±na doÄŸal, akÄ±cÄ± ve anlamlÄ± yanÄ±tlar sunabilen bir yardÄ±mcÄ± oluÅŸturmaktÄ±r.

Model, %1 oranÄ±nda LoRA yÃ¶ntemiyle optimize edilmiÅŸ ve Hugging Face platformuna aktarÄ±lmÄ±ÅŸtÄ±r:

ğŸ”— [metehanayhan/Llama3-1_Turkish_ChatBot](https://huggingface.co/metehanayhan/Llama3-1_Turkish_ChatBot)

---

## EÄŸitim Ã–zeti

| Ã–zellik | DeÄŸer |
| --- | --- |
| Temel Model | Meta LLaMA 3.1 8B |
| EÄŸitim YÃ¶ntemi | Supervised Fine-Tuning (SFT) |
| Ä°nce Ayar TekniÄŸi | LoRA |
| EÄŸitim Verisi | TÃ¼rkÃ§e EÄŸitim Q/A |
| EÄŸitim Ã–rneÄŸi SayÄ±sÄ± | 17,587 |
| EÄŸitim Epochâ€™u | 1 |
| Toplam EÄŸitim AdÄ±mÄ± (steps) | 2,199 |
| Ã–ÄŸrenme OranÄ± | 2e-5 |
| Toplam Batch Size | 8 |
| EÄŸitim SÃ¼resi (yaklaÅŸÄ±k) | 3 saat |
| EÄŸitilen Parametre OranÄ± | %1 (83M / 8B) |
| Quantization | 4-bit |

---

## Veri FormatÄ±

Veri kÃ¼mesi, her satÄ±rÄ± bir soru-cevap Ã§ifti olan TÃ¼rkÃ§e bir CSV dosyasÄ±ndan oluÅŸmaktadÄ±r. Ã–rnek:

```
soru,cevap
Ä°klim deÄŸiÅŸikliÄŸine karÅŸÄ± neler yapÄ±labilir?,
"Ä°klim deÄŸiÅŸikliÄŸiyle mÃ¼cadele iÃ§in fosil yakÄ±t kullanÄ±mÄ±nÄ±n azaltÄ±lmasÄ±, yenilenebilir enerjiye geÃ§iÅŸ gibi Ã¶nlemler alÄ±nabilir..."
```

Toplam 17,587 satÄ±r veriyle eÄŸitim gerÃ§ekleÅŸtirilmiÅŸtir.

---

## AmaÃ§ ve KullanÄ±m AlanÄ±

Model, aÅŸaÄŸÄ±daki tÃ¼rde sorulara doÄŸal dilde bilgi sunmak amacÄ±yla geliÅŸtirilmiÅŸtir:

- AÃ§Ä±klayÄ±cÄ±, Ã¶ÄŸretici, genel kÃ¼ltÃ¼r sorularÄ±
- Ã–ÄŸrencilerin veya meraklÄ± bireylerin bilgi edinme taleplerine destek
- Cevap Ã¼retimi sÄ±rasÄ±nda Ã¶zgÃ¼n, tutarlÄ± ve doÄŸal TÃ¼rkÃ§e dil kullanÄ±mÄ±

---

## HÄ±zlÄ± BaÅŸlangÄ±Ã§

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("metehanayhan/Llama3-1_Turkish_ChatBot")
model = AutoModelForCausalLM.from_pretrained("metehanayhan/Llama3-1_Turkish_ChatBot")

qa_pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
print(qa_pipe("Ä°klim deÄŸiÅŸikliÄŸi neden Ã¶nemlidir?", max_new_tokens=1024)[0]["generated_text"])
```

---

## Performans

Model, eÄŸitim verisine benzer sorularda oldukÃ§a doÄŸal, akÄ±cÄ± ve iÃ§erik aÃ§Ä±sÄ±ndan doyurucu cevaplar Ã¼retmektedir. LoRA yÃ¶ntemi sayesinde dÃ¼ÅŸÃ¼k hesaplama kaynaÄŸÄ± ile etkili bir Ã¶ÄŸrenme gerÃ§ekleÅŸtirilmiÅŸtir. EÄŸitim sÄ±rasÄ±nda gÃ¶zlemlenen bazÄ± bulgular:

- ğŸ”¸ Cevaplar TÃ¼rkÃ§e dil yapÄ±sÄ±na uygun
- ğŸ”¸ Soruyla baÄŸlamsal olarak iliÅŸkili

<p align="center">
  <img src="image.png" width="600"/>
</p>


---

## YayÄ±nlama

Model, Hugging Face Ã¼zerinde quantize edilmiÅŸ biÃ§imde (4-bit) paylaÅŸÄ±lmÄ±ÅŸtÄ±r ve inference iÃ§in kullanÄ±ma hazÄ±rdÄ±r. GGUF biÃ§imiyle de dÄ±ÅŸa aktarÄ±lmÄ±ÅŸtÄ±r.

---

## Notlar

- EÄŸitim random_state=3407 ile tekrarlanabilirlik iÃ§in sabitlenmiÅŸtir.
- EÄŸitim sÃ¼reci `Trainer` altyapÄ±sÄ±yla gerÃ§ekleÅŸtirilmiÅŸ ve standart veri Ã¶n iÅŸleme yapÄ±lmÄ±ÅŸtÄ±r.
- Model, kÃ¼Ã§Ã¼k eÄŸitimle geniÅŸ bilgi alanlarÄ±nda doÄŸal cevaplar Ã¼retme yeteneÄŸine sahiptir.

---
